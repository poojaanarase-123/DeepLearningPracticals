Practical-2 CNN_fashion_mnist_Aug13.ipynb

import matplotlib.pyplot as plt
import numpy as np
from keras.datasets import fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train.shape

x_test.shape

cat = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle_boot']
set(y_train)

plt.imshow(x_train[425], cmap = 'gray');
plt.figure(figsize=(16,10))

for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i])
    plt.xlabel(cat[y_train[i]])

x_train[1];
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train[1].shape

# add a colour channel
x_train = np.expand_dims(x_train, axis = -1)
x_test = np.expand_dims(x_test, axis = -1)
x_train[1].shape



from keras.utils import to_categorical
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
define the model architecture
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense
model = Sequential([
    Conv2D(32, (3,3), activation= 'relu', input_shape=(28, 28, 1)),
    MaxPool2D((2,2)),

    Conv2D(64, (3,3), activation= 'relu'),
    MaxPool2D((2,2)),

    Conv2D(64, (3,3), activation= 'relu'),

    Flatten(),

    Dense(64, activation= 'relu'),
    Dense(10, activation= 'softmax')
])
model.summary()

model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'],
             optimizer = 'adam')
history = model.fit(x_train, y_train, epochs = 10, batch_size = 10, 
                  validation_split= 0.2)
loss, accuracy = model.evaluate(x_test, y_test)



Practical-3 digit_multiclass_Aug13.ipynb


import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from sklearn import datasets
df = datasets.load_digits()
df;
# separate input and output

x = df.data

y = df.target
x.shape

y.shape


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

x_scaled = scaler.fit_transform(x)

from tensorflow.keras.utils import to_categorical

y_cat = to_categorical(y)


from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y_cat,
                                                   random_state= 0,
                                                   test_size= 0.2)
x_train.shape

y_train.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
model = Sequential()
model.add(Dense(64, input_shape = (x_train.shape[1],), activation = 'relu'))
model.add(Dropout(0.3))

model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.3))

model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.3))

model.add(Dense(10, activation = 'softmax'))
model.summary()


model.compile(optimizer= 'adam', loss = 'categorical_crossentropy', 
              metrics = ['accuracy'])

hist = model.fit(x_train, y_train, epochs= 30, batch_size = 32, 
                 validation_data=(x_test, y_test), verbose = 2)

model.evaluate(x_test, y_test)




Practical-4 Autoencoders.ipynb


import numpy as np
import pandas as pd
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
# Generate synthetic data
data = np.random.normal(0, 1, (1000, 20))

# Introduce some anomalies
anomalies = np.random.normal(0, 5, (50, 20))
data_with_anomalies = np.vstack([data, anomalies])
data_with_anomalies.shape

import seaborn as sns
sns.kdeplot(data)


# Normalize the data
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data_with_anomalies)

# Split the data into training and testing sets
X_train, X_test = train_test_split(data_scaled, test_size=0.2, random_state=0)
X_train.shape

X_test.shape

# Define the input layer
input_layer = Input(shape=(X_train.shape[1],))

# Encoder
encoded = Dense(16, activation='relu')(input_layer)
encoded = Dense(8, activation='relu')(encoded)
encoded = Dense(4, activation='relu')(encoded)

# Decoder
decoded = Dense(8, activation='relu')(encoded)
decoded = Dense(16, activation='relu')(decoded)
decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)

# Autoencoder model
autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()


history = autoencoder.fit(X_train, X_train, 
                          epochs=50, 
                          batch_size=10, 
                          validation_split=0.1, 
                          shuffle=True)

X_test_pred = autoencoder.predict(X_test)

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(X_test, X_test_pred, multioutput='raw_values')

# Define a threshold for anomaly detection
threshold = np.percentile(mse, 90)

# Identify anomalies
anomalies = mse > threshold
print(f"Number of anomalies detected: {np.sum(anomalies)}")


sum(mse > threshold)
2
True + True
2
import matplotlib.pyplot as plt

# Plot the loss over epochs
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()

# Plot MSE histogram
plt.hist(mse, bins=50)
plt.axvline(threshold, color='red', linestyle='--')
plt.show()



Practical-5 CBOW.ipynb


sent1 = 'they are playing on the ground from four hours'
sent2 = 'I dont know for how many hours they will be playing'
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
cv = CountVectorizer()
x_new = cv.fit_transform([sent1, sent2])
x_new

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
lines = ['It was a nice rainy day.','The things are so beatiful in his point.',
         'When your focus is clear, you won.','Many many happy returns of the day.']
tokenizer = Tokenizer()
tokenizer.fit_on_texts(lines)
tokenizer.word_docs;
tokenizer.word_index;
mat = tokenizer.texts_to_matrix(lines)
mat;
seq =  tokenizer.texts_to_sequences(lines)
seq;
padded = pad_sequences(seq, maxlen=10, padding= 'pre' )
padded;

import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
import re
import numpy as np
data = pd.read_csv("twitter_sentiments.csv", names = ['id','loc','label','text'])
data.shape


data.dtypes

data['text'] = data['text'].astype(str)

def clean_text(text):
    text = text.lower()  # lowercase
    text = re.sub(r"[^a-zA-Z]+", " ", text)  # remove non-alphanumeric characters
    return text
data["text"] = data["text"].apply(clean_text)
data



comments = data["text"].tolist()
targets = data['label'].values
np.unique(targets)
array(['Irrelevant', 'Negative', 'Neutral', 'Positive'], dtype=object)

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(comments)
sequences = tokenizer.texts_to_sequences(comments)
padded_sequences = pad_sequences(sequences, maxlen=200)
padded_sequences.shape


from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(targets)

from keras.utils import to_categorical
y_new = to_categorical(y)
y_new

from collections import Counter
Counter(targets)
Counter({'Negative': 22542,
         'Positive': 20832,
         'Neutral': 18318,
         'Irrelevant': 12990})

X_train, X_test, y_train, y_test = train_test_split(
    padded_sequences, y_new, test_size=0.2)
X_train.shape

X_test.shape


model = Sequential()
model.add(Embedding(5000, 128, input_length=200))
model.add(LSTM(64))
model.add(Dense(4, activation="softmax"))  



model.compile(loss="categorical_crossentropy", 
              optimizer="adam", metrics=["accuracy"])

model.fit(X_train, y_train, epochs=3, batch_size=32, 
          validation_data=(X_test, y_test))


new_comment = "I hate him."
new_sequence = tokenizer.texts_to_sequences([clean_text(new_comment)])
padded_new_sequence = pad_sequences(new_sequence, maxlen=200)
prediction = model.predict(padded_new_sequence)[0]
le.inverse_transform([np.argmax(prediction)])

array(['Negative'], dtype=object)



 Practical-6 Transfer Learning.ipynb


from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.resnet50 import preprocess_input
from keras.applications.resnet50 import decode_predictions
from keras.applications.resnet50 import ResNet50
from keras.utils import plot_model
image = load_img('sachin.jpg', target_size=(224,224))
image

image = img_to_array(image)
image

# reshape data for the model
image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
image.shape

# prepare the image for the ResNet50 model
image = preprocess_input(image)
# load the model
model = ResNet50()
plot_model(model)

# predict the probability across all output classes
yhat = model.predict(image)

yhat

import numpy as np
np.argmax(yhat)

decode_predictions(yhat)[0][0][1]
'football_helmet'
from keras.datasets import cifar10
import matplotlib.pyplot as plt
(x_train,y_train),(x_test,y_test) = cifar10.load_data()
x_train.shape

labels = ['airplane','automobile','bird','cat','deer','dog',
          'frog','horse','ship','truck']
plt.figure(figsize=(16,16))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.axis('off')
    plt.imshow(x_train[i])
    plt.title(labels[y_train[i][0]])

resnet50 = ResNet50(include_top=False, weights='imagenet',
                    input_shape=(32, 32, 3))

plot_model(resnet50)

from keras.layers import Flatten, Dense
#Flatten output layer of Resnet
flattened = Flatten()(resnet50.output)

#Fully connected layer 1
fc1 = Dense(128, activation='relu', name="AddedDense1")(flattened)

#Fully connected layer, output layer
fc2 = Dense(10, activation='softmax', name="AddedDense2")(fc1)
resnet50.trainable
True
resnet50.trainable = False
from keras.models import Model
model = Model(inputs=resnet50.inputs, outputs=fc2)
plot_model(model)

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=2)

image = load_img('cat.jpg', target_size=(32,32))
img_array = img_to_array(image)
img_new = img_array.reshape(1,32,32,3)
# detect the object
y = model.predict(img_new)

labels[np.argmax(y)+1]
'cat'
 
 